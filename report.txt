# Chess assignment report

(Replace the square-bracketed text with your own text. *Leave everything else unchanged.* Note, the reports are parsed to check word limits, etc. Changing the format may cause the parsing to fail.)

## Feature Extraction (Max 200 Words)

Each board image is initially splitted into 64 sqaures and each square of 
50x50 pixels is passed into a numpy array and is flattened into an array, 
to obtain 2500 feature vectors.

I have used Principal Component Analysis (PCA) to reduce the dimension of the 
feature vector down to 10. A covariance matrix of the dataset is calculated, 
in which 10 principle axes with the 10 biggest eigenvalues is obtained, which 
will represent the original data the most accurately after performing matrix 
multiplication on the original data.

## Classifier (Max 200 Words)

I initially used a Gaussian distribution approach because I was most familiar 
with the method. But after thinking about whether using a k-nearest neighbour 
classifier would have been a better approach, I came up with the hypothesis 
that it will be, since a piece can be on both black and white squares, but with 
the Gaussian approach, this was not put into consideration. And we don’t want 
to further split the dataset by the classes AND the color of a square, we would 
end up with twice the number of Gaussian distribution models with less training 
data used for each, resulting in less accuracy, further splitting a class up 
doesn’t make any sense in the first place. Whereas a k-NN is able to separate the 
classes very well with a distance function, even if the pieces are on different 
colour squares.

So I decided to implement a k-NN with an euclidean distance measure to further 
test my hypothesis, and saw an 1% increase in accuracy for clean data, and a 
0.9% increase for noisy data, for k=8.

I also decided to change the classification strategy of k-NN from using a vote 
system to a weighting one which is more accurate, since we are using a small 
value for k, there's more likelihood of ties in votes.

## Full Board Classification (Max 200 Words)

An idea that I came up with was
    - construct a list of lists of size 64, each list stores every label 
        (include duplicates) thats occurred in a particular position
    - multiply the weightings of each unique class in k-NN samples by the 
        probability of occurence of each class + 1
    - (add 1 so we don't totally eliminate an probability by multiplying by 0, 
        if there were no occurence of a piece on a particular position)
    - classify squares according to modified probabilities

After implmenting this idea, I was able to increase the board accuracy, however 
this also caused the square accuracy to decrease.

## Performance

My percentage correctness scores (to 1 decimal place) for the development data are as follows.

Clean data:

- Square mode: 98.4%
- Board mode: 98.5%

Noisy data:

- Square mode: 94.2%
- Board mode: 94.8%

## Other information (Optional, Max 100 words)

I couldn't figure out why but, the classification system gives different results 
everytime I run it, so when the code is ran on the given data, it may yield 
slightly different accuracies (+-0.2%) from the ones stated above.